from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_groq import ChatGroq
from langchain_core.output_parsers import StrOutputParser
import os
from dotenv import load_dotenv

load_dotenv()

# Get API key with fallback
groq_api_key = os.getenv("GROQ_API_KEY")
if not groq_api_key:
    print("Warning: GROQ_API_KEY not found in environment variables")
    groq_api_key = "dummy_key"  # Fallback to prevent crashes

# Initialize model with error handling
try:
    model = ChatGroq(model="Gemma2-9b-It", groq_api_key=groq_api_key)
except Exception as e:
    print(f"Error initializing ChatGroq model: {e}")
    model = None

# ğŸ‘‡ Ø§Ø³ØªØ®Ø¯Ù… prompt Ø¨Ù…ØªØºÙŠØ±Ø§Øª Ù…ØªÙˆØ§ÙÙ‚Ø©
prompt = ChatPromptTemplate.from_messages([
    ("system", "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø²Ø±Ø§Ø¹ÙŠ Ø®Ø¨ÙŠØ±. Ø£Ø¬Ø¨ Ø¨Ø§Ø­ØªØ±Ø§Ù ÙˆØ¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

# Ù…Ø¤Ù‚ØªÙ‹Ø§ Ù†Ø®Ø²Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ù‡Ù†Ø§ (Ù…Ù…ÙƒÙ† ØªØ­ÙˆÙŠÙ„Ù‡ Ù„ Redis Ø£Ùˆ DB Ù„Ø§Ø­Ù‚Ù‹Ø§)
chat_memory_store = {}

# runnable history chain - only create if model is available
chat_chain = None
if model is not None:
    chat_chain = RunnableWithMessageHistory(
        prompt | model | StrOutputParser(),
        lambda session_id: chat_memory_store.setdefault(session_id, ChatMessageHistory()),
        input_messages_key="input",
        history_messages_key="history",
    )
